# -*- coding: utf-8 -*-
"""Deep learning_3units.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ciDk921TaH46uVS9znrIoylgjHMf60O
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy import stats
from sklearn import metrics
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
import os

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from keras.utils import to_categorical
import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from tensorflow.keras.layers import Conv2D, MaxPool2D
from tensorflow.keras.optimizers import Adam

#lettura del dataset con tre unità
df = pd.read_csv('tot_complete.csv', sep=",|:", header=None, engine='python')
#dare nome alle colonne
df.columns = [
    'quat_1t','quat_2t','quat_3t','quat_4t', #quaternioni torace
           'quat_1a','quat_2a','quat_3a','quat_4a', #quaternioni addome
           'quat_1r','quat_2r','quat_3r','quat_4r', #quaternioni reference
           'user','activity']

#calcolo del numero di campioni per ogni attività
df_shape = df.shape
print("The data has Rows {}, Columns {}".format(df_shape[0], df_shape[1]))
sns.catplot(x="activity", kind="count", data=df, height=5, aspect=4, palette="Set3")
#conversione da obj a numeric
df['quat_1t'] = pd.to_numeric(df['quat_1t'],errors='coerce')
df['quat_2t'] = pd.to_numeric(df['quat_1t'],errors='coerce')
df['quat_3t'] = pd.to_numeric(df['quat_1t'],errors='coerce')
df['quat_4t'] = pd.to_numeric(df['quat_1t'],errors='coerce')
df['quat_1a'] = pd.to_numeric(df['quat_1a'],errors='coerce')
df['quat_2a'] = pd.to_numeric(df['quat_2a'],errors='coerce')
df['quat_3a'] = pd.to_numeric(df['quat_3a'],errors='coerce')
df['quat_4a'] = pd.to_numeric(df['quat_4a'],errors='coerce')
df['quat_1r'] = pd.to_numeric(df['quat_1r'],errors='coerce')
df['quat_2r'] = pd.to_numeric(df['quat_2r'],errors='coerce')
df['quat_3r'] = pd.to_numeric(df['quat_3r'],errors='coerce')
df['quat_4r'] = pd.to_numeric(df['quat_4r'],errors='coerce')

#per bilanciare il dataset si cerca l'attività con il numero di sample minore
df['activity'].value_counts()

#tutte le altre attivià vengono tagliate per avere quel numero di sample
sitting= df[df['activity']=='sitting'].head(34796).copy()
supine = df[df['activity']=='supine'].head(34796).copy()
prone = df[df['activity']=='prone'].head(34796).copy()
lying_left = df[df['activity']=='lying_left'].head(34796).copy()
lying_right = df[df['activity']=='lying_right'].head(34796).copy()
standing = df[df['activity']=='standing'].head(34796).copy()
stairs = df[df['activity']=='stairs'].copy()
walking = df[df['activity']=='walking'].head(34796).copy()
running = df[df['activity']=='running'].head(34796).copy()
cyclette = df[df['activity']=='cyclette'].head(34796).copy()

# salvataggio dataset bilanciato
balanced_data = pd.DataFrame()
balanced_data = balanced_data.append([sitting, supine, prone, lying_left, lying_right, standing, stairs, walking, running, cyclette])
# verifica che tutte le attività abbiano uguale numero di sample
#balanced_data['activity'].value_counts()

balanced_data["activity"] = balanced_data["activity"].astype("|S")
# hot encoding
label_encoder = LabelEncoder()
balanced_data["activitynum"] = label_encoder.fit_transform(balanced_data["activity"])
#standardization
quaternion = balanced_data[['quat_1t','quat_2t','quat_3t','quat_4t', 'quat_1a','quat_2a','quat_3a','quat_4a', 'quat_1r','quat_2r','quat_3r','quat_4r']]
scaler = StandardScaler()
quaternion = scaler.fit_transform(quaternion)
quaternion = pd.DataFrame(quaternion, columns = ['quat_1t','quat_2t','quat_3t','quat_4t', 'quat_1a','quat_2a','quat_3a','quat_4a', 'quat_1r','quat_2r','quat_3r','quat_4r'])
#segmentation
N_TIME_STEPS = 200
N_FEATURES = 12
step = 20
segments = []
labels = []
for i in range(0, len(quaternion) - N_TIME_STEPS, step):
    quat_1t = quaternion['quat_1t'].values[i: i + N_TIME_STEPS]
    quat_2t = quaternion['quat_2t'].values[i: i + N_TIME_STEPS]
    quat_3t = quaternion['quat_3t'].values[i: i + N_TIME_STEPS]
    quat_4t = quaternion['quat_4t'].values[i: i + N_TIME_STEPS]
    quat_1a = quaternion['quat_1a'].values[i: i + N_TIME_STEPS]
    quat_2a = quaternion['quat_2a'].values[i: i + N_TIME_STEPS]
    quat_3a = quaternion['quat_3a'].values[i: i + N_TIME_STEPS]
    quat_4a = quaternion['quat_4a'].values[i: i + N_TIME_STEPS]
    quat_1r = quaternion['quat_1r'].values[i: i + N_TIME_STEPS]
    quat_2r = quaternion['quat_2r'].values[i: i + N_TIME_STEPS]
    quat_3r = quaternion['quat_3r'].values[i: i + N_TIME_STEPS]
    quat_4r = quaternion['quat_4r'].values[i: i + N_TIME_STEPS]
    label = stats.mode(balanced_data['activitynum'][i: i + N_TIME_STEPS])[0][0]
    segments.append([
                     quat_1t, quat_2t, quat_3t, quat_4t, 
                     quat_1a, quat_2a, quat_3a, quat_4a, quat_1r, quat_2r, quat_3r, quat_4r])
    labels.append(label)
#è necessario modellare i segmenti in una forma migliore
segments = np.asarray(segments).reshape(-1, N_TIME_STEPS , N_FEATURES)
labels = np.asarray(labels)

X = segments #attributi
X = np.asarray(X).astype('float32')
y = labels
y = to_categorical(y)

#train e test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, 
                                                    random_state = 42, 
)
#calcolo di alcuni parametri utili
training_data_count = len(X_train)  # 18732 training series 
test_data_count = len(X_test)  # 4683 testing series
n_steps = len(X_train[0])  # 200 timesteps per series
n_input = len(X_train[0][0])  # 12 input parameters per timestep
n_output = (y_train[1])

"""1DCNN"""

model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(200,12)))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(Dropout(0.5))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Compiling the model
model.compile(optimizer='rmsprop', 
              loss = 'sparse_categorical_crossentropy', 
              metrics = ['accuracy'])

#percorso per salvare il modello migliore in .h5
checkpoint_filepath = "working/model_1D_{val_accuracy:.3f}.h5"
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy', patience=5)

# Training the model
history = model.fit(X_train, y_train,
                    epochs = 10, 
                    batch_size = 16,
                    validation_data= (X_test, y_test), 
                    shuffle=True,
                    callbacks=[model_checkpoint_callback,
                               early_stopping_callback])

validation_acc = np.amax(history.history['val_accuracy'])
print('Best validation accuracy:', validation_acc)

"""2DCNN"""

print(X_train.shape)
print(X_test.shape)

X_train = X_train.reshape(12171, 200, 12, 1)
X_test = X_test.reshape(5217, 200, 12, 1)

model = Sequential()
model.add(Conv2D(11, (2, 2), activation = 'relu', input_shape = X_train[0].shape))
model.add(Dropout(0.1)) 
model.add(Conv2D(32, (2, 2), activation='relu'))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

model.summary()

model.compile(optimizer='rmsprop', 
              loss = 'sparse_categorical_crossentropy', 
              metrics = ['accuracy'])

checkpoint_filepath = "working/model_2D_{val_accuracy:.3f}.h5"
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)
#early_stopping_callback = tf.keras.callbacks.EarlyStopping(
#    monitor='val_accuracy', patience=5)

history = model.fit(X_train, y_train,
                    epochs = 15, 
                    batch_size = 16,
                    validation_data= (X_test, y_test), 
                    shuffle=True,
                    callbacks=[model_checkpoint_callback])

validation_acc = np.amax(history.history['val_accuracy'])
print('Best validation accuracy:', validation_acc)

"""BASE LSTM"""

from keras.layers import LSTM

X_train = X_train.reshape(12171, 200, 12)
X_test = X_test.reshape(5217, 200, 12)

model = Sequential()
model.add(LSTM(100, input_shape=(200, 12)))
model.add(Dropout(0.5))
model.add(Dense(12, activation='sigmoid'))
model.summary()

model.compile(loss='sparse_categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

checkpoint_filepath = "working/model_BaseLSTM_{val_accuracy:.3f}.h5"
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy', patience=5)

history = model.fit(X_train,y_train,
          batch_size=16,
          validation_data=(X_test, y_test),
          epochs=25,shuffle=True,
          callbacks=[model_checkpoint_callback, early_stopping_callback])

validation_acc = np.amax(history.history['val_accuracy'])
print('Best validation accuracy:', validation_acc)

"""MULTI LSTM"""

model = Sequential()
model.add(LSTM(32,return_sequences=True,input_shape=(200, 12)))
model.add(Dropout(0.5))
model.add(LSTM(28,input_shape=(200, 12)))
model.add(Dropout(0.6))
model.add(Dense(10, activation='softmax'))
model.summary()

model.compile(loss='sparse_categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

checkpoint_filepath = "working/model_multi_LSTM_{val_accuracy:.3f}.h5"
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

# Training the model
history = model.fit(X_train,
          y_train,
          batch_size=16,
          validation_data=(X_test, y_test),
          epochs=50,
          callbacks=[model_checkpoint_callback])

validation_acc = np.amax(history.history['val_accuracy'])
print('Best validation accuracy:', validation_acc)

"""GRU"""

from keras.layers import GRU

model = Sequential()
model.add(GRU(150, input_shape=(200, 12)))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))
model.summary()

model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

checkpoint_filepath = "working/model_GRU_{val_accuracy:.3f}.h5"
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

# Training the model
history = model.fit(X_train,y_train,
          batch_size=16,
          validation_data=(X_test, y_test),
          epochs=16,shuffle=True,
          callbacks=[model_checkpoint_callback])

validation_acc = np.amax(history.history['val_accuracy'])
print('Best validation accuracy:', validation_acc)

"""report + cm"""
# viene caricato il modello da analizzare
from keras.models import load_model
test_model = load_model('working/model_BaseLSTM_0.973.h5')
#predizione
y_pred = test_model.predict(X_test, steps=1, verbose=0)
rounded_y_pred = np.argmax(y_pred, axis=-1)
#report con precision,recall e f1score
print(classification_report(y_true=y_test, y_pred=rounded_y_pred))
#labels per la confusion matrix
labels = ['cyclette', 'lying_left', 'lying_right', 'prone', 'running', 
          'sitting'
          'stairs', 'standing', 'supine', 
          'walking']

cm = confusion_matrix(y_true=y_test, y_pred=rounded_y_pred)
precision = cm/cm.sum(axis = 0)
recall = (cm.T/cm.sum(axis = 1)).T
sns.set(font_scale=1.5)

# per avere la confusion matrix standardizzata
#cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
plt.figure(figsize=(16,7))
sns.heatmap(cm, cmap = "Blues", annot = True, fmt = ".1f", xticklabels=labels, yticklabels=labels)
plt.title("Confusion Matrix", fontsize = 30)
plt.xlabel('Predicted Class', fontsize = 20)
plt.ylabel('Original Class', fontsize = 20)
plt.tick_params(labelsize = 15)
plt.xticks(rotation = 90)
plt.show()
# matrice di precisione
plt.figure(figsize=(16,7))
sns.heatmap(precision, cmap = "Blues", annot = True, fmt = ".2f", xticklabels=labels, yticklabels=labels)
plt.title("Precision Matrix", fontsize = 30)
plt.xlabel('Predicted Class', fontsize = 20)
plt.ylabel('Original Class', fontsize = 20)
plt.tick_params(labelsize = 15)
plt.xticks(rotation = 90)
plt.show()
    #matrice di recall
plt.figure(figsize=(16,7))
sns.heatmap(recall, cmap = "Blues", annot = True, fmt = ".2f", xticklabels=labels, yticklabels=labels)
plt.title("Recall Matrix", fontsize = 30)
plt.xlabel('Predicted Class', fontsize = 20)
plt.ylabel('Original Class', fontsize = 20)
plt.tick_params(labelsize = 15)
plt.xticks(rotation = 90)
plt.show()