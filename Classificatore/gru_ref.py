# -*- coding: utf-8 -*-
"""GRU_ref.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E_CvsqCyrWEmTvOYlcwwDPkQannD7LR9
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from keras.layers import GRU
from keras.utils import to_categorical
import tensorflow as tf

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout

import io
import seaborn as sns
import itertools
from scipy import stats
from sklearn.model_selection import train_test_split

from google.colab import files
uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded['preprocessref_reduced.txt']))

N_TIME_STEPS = 200
N_FEATURES = 4
step = 20
segments = []
labels = []
for i in range(0, len(df) - N_TIME_STEPS, step):
    quat_1 = df['quat1'].values[i: i + N_TIME_STEPS]
    quat_2 = df['quat2'].values[i: i + N_TIME_STEPS]
    quat_3 = df['quat3'].values[i: i + N_TIME_STEPS]
    quat_4 = df['quat4'].values[i: i + N_TIME_STEPS]
    label = stats.mode(df['activity'][i: i + N_TIME_STEPS])[0][0]
    segments.append([quat_1, quat_2, quat_3, quat_4])
    labels.append(label)

segments = np.asarray(segments).reshape(-1, N_TIME_STEPS , N_FEATURES)
labels = np.asarray(labels)
X = segments
y = labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.fit_transform(y_test)

label_encoder.classes_

# Initializing parameters
batch_size = 16
n_hidden = 150

timesteps = len(X_train[0])
input_dim = len(X_train[0][0])
n_classes = 10
print(timesteps)
print(input_dim)
print(len(X_train))
print(n_classes)

"""# GRU_RNN

"""

# Initiliazing the sequential model
model = Sequential()
# Configuring the parameters
model.add(GRU(n_hidden, input_shape=(timesteps, input_dim)))
# Adding a dropout layer
model.add(Dropout(0.5))
# Adding a dense output layer with sigmoid activation
model.add(Dense(n_classes, activation='softmax'))
model.summary()

#Compiling the model
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

checkpoint_filepath = "working/model_{val_accuracy:.3f}.h5"
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy', patience=5)

# Training the model
history = model.fit(X_train,y_train,
          batch_size=batch_size,
          validation_data=(X_test, y_test),
          epochs=150,shuffle=True,
          callbacks=[model_checkpoint_callback,early_stopping_callback])

validation_acc = np.amax(history.history['val_accuracy'])
print('Best validation accuracy:', validation_acc)

from keras.models import load_model
test_model = load_model('complete_GRU.h5')

y_pred = test_model.predict(X_test, steps=1, verbose=0)

rounded_y_pred = np.argmax(y_pred, axis=-1)

print(classification_report(y_true=y_test, y_pred=rounded_y_pred))

labels = ['cyclette', 'lying_left', 'lying_right', 'prone', 'running', 
          #'sitting_with_support', 'sitting_without_support', 
          'sitting',
          'stairs', 'standing', 'supine', 
          'walking']
          #'walking_fast', 'walkig_slow']

cm = confusion_matrix(y_true=y_test, y_pred=rounded_y_pred)

#cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
print(cm)

def print_confusionMatrix(y_test, rounded_y_pred):
  cm = confusion_matrix(y_test, rounded_y_pred)
  

precision = cm/cm.sum(axis = 0)
recall = (cm.T/cm.sum(axis = 1)).T
sns.set(font_scale=1.5)
#cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
plt.figure(figsize=(16,7))
sns.heatmap(cm, cmap = "Blues", annot = True, fmt = ".1f", xticklabels=labels, yticklabels=labels)
plt.title("Confusion Matrix", fontsize = 30)
plt.xlabel('Predicted Class', fontsize = 20)
plt.ylabel('Original Class', fontsize = 20)
plt.tick_params(labelsize = 15)
plt.xticks(rotation = 90)
plt.show()
    
print("-"*125)


plt.figure(figsize=(16,7))
sns.heatmap(precision, cmap = "Blues", annot = True, fmt = ".2f", xticklabels=labels, yticklabels=labels)
plt.title("Precision Matrix", fontsize = 30)
plt.xlabel('Predicted Class', fontsize = 20)
plt.ylabel('Original Class', fontsize = 20)
plt.tick_params(labelsize = 15)
plt.xticks(rotation = 90)
plt.show()
    
print("-"*125)

plt.figure(figsize=(16,7))
sns.heatmap(recall, cmap = "Blues", annot = True, fmt = ".2f", xticklabels=labels, yticklabels=labels)
plt.title("Recall Matrix", fontsize = 30)
plt.xlabel('Predicted Class', fontsize = 20)
plt.ylabel('Original Class', fontsize = 20)
plt.tick_params(labelsize = 15)
plt.xticks(rotation = 90)
plt.show()

def plot_learningCurve(history, epochs):
  # Plot training & validation accuracy values
  epoch_range = range(1, epochs+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc='upper left')
  plt.show()

  # Plot training & validation loss values
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc='upper left')
  plt.show()

plot_learningCurve(history, 31)